{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e49442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cseadmin/whj/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from Eearly_stop import *\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src_py.cpmix_utils import preprocess_data\n",
    "from src_py.rhorho import RhoRhoEvent\n",
    "from src_py.a1a1 import A1A1Event\n",
    "from src_py.a1rho import A1RhoEvent\n",
    "from src_py.data_utils import read_np, EventDatasets\n",
    "from src_py.process_background import *\n",
    "import train_rhorho, train_a1rho, train_a1a1\n",
    "from src_py.metrics_utils import calculate_deltas_unsigned, calculate_deltas_signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e91a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da403d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decaymodes = ['rhorho', 'a1rho', 'a1a1']\n",
    "types = {\"nn_rhorho\": train_rhorho.start,\"nn_a1rho\": train_a1rho.start,\"nn_a1a1\": train_a1a1.start}\n",
    "parser = argparse.ArgumentParser(description='Train classifier')\n",
    "\n",
    "parser.add_argument(\"--num_classes\", dest=\"NUM_CLASSES\", type=int, default=11)\n",
    "parser.add_argument(\"-l\", \"--layers\", dest=\"LAYERS\", type=int, help = \"number of NN layers\", default=6)\n",
    "parser.add_argument(\"-s\", \"--size\", dest=\"SIZE\", type=int, help=\"NN size\", default=100)\n",
    "parser.add_argument(\"-lambda\", \"--lambda\", type=float, dest=\"LAMBDA\", help=\"value of lambda parameter\", default=0.0)\n",
    "parser.add_argument(\"-m\", \"--method\", dest=\"METHOD\", choices=[\"A\", \"B\", \"C\"], default=\"A\")\n",
    "parser.add_argument(\"-o\", \"--optimizer\", dest=\"OPT\", \n",
    "    choices=[\"GradientDescentOptimizer\", \"AdadeltaOptimizer\", \"AdagradOptimizer\",\n",
    "         \"ProximalAdagradOptimizer\", \"AdamOptimizer\", \"FtrlOptimizer\",\n",
    "         \"ProximalGradientDescentOptimizer\", \"RMSPropOptimizer\"], default=\"AdamOptimizer\")\n",
    "parser.add_argument(\"-d\", \"--dropout\", dest=\"DROPOUT\", type=float, default=0.0)\n",
    "parser.add_argument(\"-e\", \"--epochs\", dest=\"EPOCHS\", type=int, default=3)\n",
    "# parser.add_argument(\"-f\", \"--features\", dest=\"FEAT\", help=\"Features\", default=\"Variant-All\")\n",
    "# #         choices= [\"Variant-All\", \"Variant-1.0\", \"Variant-1.1\", \"Variant-2.0\", \"Variant-2.1\",\n",
    "# #                   \"Variant-2.2\", \"Variant-3.0\", \"Variant-3.1\", \"Variant-4.0\", \"Variant-4.1\"])\n",
    "\n",
    "########### Change this to according dir to download data #######################\n",
    "parser.add_argument(\"-i\", \"--input\", dest=\"IN\", default='HiggsCP_data/rhorho')\n",
    "\n",
    "parser.add_argument(\"--miniset\", dest=\"MINISET\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--z_noise_fraction\", dest=\"Z_NOISE_FRACTION\", type=float, default=0.0)\n",
    "\n",
    "parser.add_argument(\"--delt_classes\", dest=\"DELT_CLASSES\", type=int, default=0,\n",
    "                    help='Maximal distance between predicted and valid class for event being considered as correctly classified')\n",
    "\n",
    "parser.add_argument(\"--unweighted\", dest=\"UNWEIGHTED\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--reuse_weights\", dest=\"REUSE_WEIGHTS\", type=bool, default=False)\n",
    "parser.add_argument(\"--restrict_most_probable_angle\", dest=\"RESTRICT_MOST_PROBABLE_ANGLE\", type=bool, default=False)\n",
    "parser.add_argument(\"--force_download\", dest=\"FORCE_DOWNLOAD\", type=bool, default=False)\n",
    "parser.add_argument(\"--normalize_weights\", dest=\"NORMALIZE_WEIGHTS\", type=bool, default=False)\n",
    "\n",
    "\n",
    "parser.add_argument(\"--beta\",  type=float, dest=\"BETA\", help=\"value of beta parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_b\", type=float, dest=\"pol_b\", help=\"value of b parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_c\", type=float, dest=\"pol_c\", help=\"value of c parameter for polynomial smearing\", default=0.0)\n",
    "\n",
    "parser.add_argument(\"--w1\", dest=\"W1\")\n",
    "parser.add_argument(\"--w2\", dest=\"W2\")\n",
    "parser.add_argument(\"--f\", dest=\"FEAT\", default=\"Variant-All\")\n",
    "parser.add_argument(\"--plot_features\", dest=\"PLOT_FEATURES\", choices=[\"NO\", \"FILTER\", \"NO-FILTER\"], default=\"NO\")\n",
    "parser.add_argument(\"--training_method\", dest=\"TRAINING_METHOD\", choices=[\"soft_weights\", \"soft_c012s\",  \"soft_argmaxs\", \"regr_c012s\", \"regr_weights\", \"regr_argmaxs\"], default=\"soft_weights\")\n",
    "parser.add_argument(\"--hits_c012s\", dest=\"HITS_C012s\", choices=[\"hits_c0s\", \"hits_c1s\",  \"hits_c2s\"], default=\"hits_c0s\")\n",
    "\n",
    "######Change this to according type (rhorho, a1rho, a1a1)#######################\n",
    "parser.add_argument(\"-t\", \"--type\", dest=\"TYPE\", choices=types.keys(), default='nn_rhorho')\n",
    "\n",
    "parser.add_argument(\"-r\", \"--reprocess\", dest=\"REPRO\", type=bool, default=False)\n",
    "args, unknown = parser.parse_known_args()\n",
    "parser.add_argument(\"-bkgd\", \"--bkgdpath\", dest=\"BKGDPATH\", default= 'Ztt_dataset_Elz/pythia.Z_115_135.%s.1M.*.outTUPLE_labFrame')\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea597406",
   "metadata": {},
   "source": [
    "### Preprocessing signal samples from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a341cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    for decaymode in tqdm(decaymodes):\n",
    "        points = []\n",
    "        args.IN = 'HiggsCP_data/'+decaymode\n",
    "        args.TYPE = 'nn_'+decaymode\n",
    "        data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "        event = eval(events[args.TYPE])(data, args)\n",
    "        points.append(EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED))\n",
    "        pickle.dump(points,open(args.IN+'/events_wo_background.pk','wb'))\n",
    "        break # only precessing the rhorho data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position={'nn_rhorho':[0,1,4,5,6,7], 'nn_a1rho':[0,1,2,3,5,6,7], 'nn_a1a1':[0,1,2,3,5,6,8,9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cee035",
   "metadata": {},
   "source": [
    "### Loading signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "points=pickle.load(open(args.IN+'/events_wo_background.pk','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e548b8b",
   "metadata": {},
   "source": [
    "### Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47078747",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39972924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, rhorho_data_mc,rhorho_data_true,rhorho_labels_mc,rhorho_labels_true):\n",
    "        self.rhorho_data_mc = torch.from_numpy(rhorho_data_mc).float().to(device)\n",
    "        self.rhorho_data_true = torch.from_numpy(rhorho_data_true).float().to(device)\n",
    "        \n",
    "        self.rhorho_labels_mc =torch.from_numpy(rhorho_labels_mc).float().to(device)\n",
    "        self.rhorho_labels_true =torch.from_numpy(rhorho_labels_true).float().to(device)\n",
    "    def __getitem__(self, index):\n",
    "        return self.rhorho_data_mc[index],self.rhorho_data_true[index],self.rhorho_labels_mc[index],self.rhorho_labels_true[index]\n",
    "    def __len__(self):\n",
    "        return min(len(self.rhorho_labels_mc),len(self.rhorho_labels_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_idx=0\n",
    "\n",
    "mc_train_idx=np.random.choice(np.arange(points[particle_idx].train.x.shape[0]),int(points[particle_idx].train.x.shape[0]*0.5),replace=False)\n",
    "true_train_idx=list(set(np.arange(points[particle_idx].train.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_valid_idx=np.random.choice(np.arange(points[particle_idx].valid.x.shape[0]),int(points[particle_idx].valid.x.shape[0]*0.5),replace=False)\n",
    "true_valid_idx=list(set(np.arange(points[particle_idx].valid.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_test_idx=np.random.choice(np.arange(points[particle_idx].test.x.shape[0]),int(points[particle_idx].test.x.shape[0]*0.5),replace=False)\n",
    "true_test_idx=list(set(np.arange(points[particle_idx].test.x.shape[0]))-set(mc_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty=0.0\n",
    "\n",
    "train_datasets = MyDataset(points[particle_idx].train.x[mc_train_idx], points[particle_idx].train.x[true_train_idx]+uncertainty*np.random.normal(0,1,size=points[particle_idx].train.x[true_train_idx].shape),\n",
    "                          points[particle_idx].train.weights[mc_train_idx],points[particle_idx].train.weights[true_train_idx])\n",
    "train_loader = DataLoader(dataset = train_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "valid_datasets = MyDataset(points[particle_idx].valid.x[mc_valid_idx], points[particle_idx].valid.x[true_valid_idx]+uncertainty*np.random.normal(0,1,size=points[particle_idx].valid.x[true_valid_idx].shape),\n",
    "                          points[particle_idx].valid.weights[mc_valid_idx],points[particle_idx].valid.weights[true_valid_idx])\n",
    "valid_loader = DataLoader(dataset = valid_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "test_datasets = MyDataset(points[particle_idx].test.x[mc_test_idx], points[particle_idx].test.x[true_test_idx]+uncertainty*np.random.normal(0,1,size=points[particle_idx].test.x[true_test_idx].shape),\n",
    "                          points[particle_idx].test.weights[mc_test_idx],points[particle_idx].test.weights[true_test_idx])\n",
    "test_loader = DataLoader(dataset = test_datasets,batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34476c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_layers=1, size=100, lr=1e-3, drop_prob=0, inplace=False, \n",
    "                 tloss=\"regr_weights\", activation='linear', input_noise=0.0, optimizer=\"AdamOptimizer\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_features,size,bias=False)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.extend([nn.Linear(size,size,bias=False),\n",
    "                           nn.BatchNorm1d(size),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(drop_prob, inplace)\n",
    "                          ])\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "        self.linear2 = nn.Linear(size,num_classes,bias=False)\n",
    "        self.linear3 = nn.Linear(size,2,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        out = self.linear2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_features=points[particle_idx].train.x.shape[1], num_classes=args.NUM_CLASSES,num_layers=args.LAYERS,drop_prob=0).to(device)\n",
    "opt_g=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True,path='model/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691c833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch=200\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    train_loss,sample_numbers,acc,total_samples,bg_acc=0,0,0,0,0\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,_) in enumerate(train_loader):\n",
    "        \n",
    "        opt_g.zero_grad()\n",
    "        rhorho_s=rhorho_s[label_s.sum(axis=1)!=0]\n",
    "        label_s=label_s[label_s.sum(axis=1)!=0]\n",
    "        outputs=model(rhorho_s)\n",
    "        if isinstance(criterion,nn.CrossEntropyLoss):\n",
    "            loss=criterion(outputs,torch.argmax(label_s,axis=1))\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            acc+=(predictions==torch.argmax(label_s,axis=1)).sum().item()\n",
    "        else:\n",
    "            loss=criterion(outputs,label_s)\n",
    "        loss.backward()\n",
    "        train_loss+=loss.item()*len(rhorho_s)\n",
    "        sample_numbers+=len(rhorho_s)\n",
    "        opt_g.step()\n",
    "        print('\\r training loss: %.3f \\t acc: %.3f \\t' %(train_loss/sample_numbers,acc/sample_numbers),end='')\n",
    "    print()\n",
    "    vaild_acc,vaild_numbers,total_samples,bg_acc=0,0,0,0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (rhorho_s,rhorho_t,label_s,label_t) in enumerate(valid_loader):\n",
    "            total_samples+=len(rhorho_t)\n",
    "            rhorho_t= rhorho_t[label_t.sum(axis=1)!=0]\n",
    "            label_t = label_t[label_t.sum(axis=1)!=0]\n",
    "            outputs=model(rhorho_t)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            vaild_acc+=(predictions==torch.argmax(label_t,axis=1)).sum().item()\n",
    "            vaild_numbers+=len(rhorho_t)\n",
    "    print()\n",
    "    print('\\r  acc: %.3f \\t ' %(vaild_acc/vaild_numbers),end='')\n",
    "    print()\n",
    "    early_stopping(-vaild_acc/vaild_numbers,model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break;\n",
    "# test_loss=0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, label in test_loader:\n",
    "#         outputs=model(inputs)\n",
    "#         test_loss+=mse_loss(outputs,label).item()*len(inputs)\n",
    "#     print('test loss: %f' %(test_loss/len(test_loader.dataset.tensors[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eb181",
   "metadata": {},
   "source": [
    "### Converting bkgd raw data into npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a605ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579372f5135445f6bcf8445afb4c29ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dccc0eb126424a9ff9b9f721e66141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae997a264c0480cb4d8126ec2c4a39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ztt raw data already converted into npy file :)\n"
     ]
    }
   ],
   "source": [
    "convert_bkgd_raw(args.BKGDPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1555fd9",
   "metadata": {},
   "source": [
    "### Preprocessing singal and bkgd from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30480ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    for decaymode in tqdm(decaymodes):\n",
    "        points = []\n",
    "        args.Z_NOISE_FRACTION = 1\n",
    "        args.IN = 'HiggsCP_data/'+decaymode\n",
    "        args.TYPE = 'nn_'+decaymode\n",
    "        data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "        event = eval(events[args.TYPE])(data, args)\n",
    "        points.append(EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED))\n",
    "        pickle.dump(points,open(args.IN+'/events_w_background.pk','wb'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e77b68",
   "metadata": {},
   "source": [
    "### Loading bkgd samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_points=pickle.load(open(args.IN+'/events_w_background.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=[]\n",
    "background.append(background_points[particle_idx].train.x[background_points[particle_idx].train.weights.sum(axis=1)==0])\n",
    "background.append(background_points[particle_idx].valid.x[background_points[particle_idx].valid.weights.sum(axis=1)==0])\n",
    "background.append(background_points[particle_idx].test.x[background_points[particle_idx].test.weights.sum(axis=1)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=np.concatenate(background)\n",
    "print(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99964c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=torch.tensor(background).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3885c6",
   "metadata": {},
   "source": [
    "### Testing NN w/ bkgd only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5c4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/best_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs=[]\n",
    "    for i in tqdm(range(0, 400000,batch_size)):\n",
    "        outputs.append(model(background[i:i+batch_size]).detach().cpu())\n",
    "outputs=torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47decf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_outputs=torch.argmax(torch.softmax(outputs,axis=1),axis=1).numpy()\n",
    "bg_labels_counts=np.unique(bg_outputs,return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5464f5",
   "metadata": {},
   "source": [
    "### Testing NN w/ signal only (Class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/best_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    signal_outputs,signal_labels=[],[]\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,_) in enumerate(train_loader):\n",
    "        signal_outputs.append(model(rhorho_s).detach().cpu())\n",
    "        signal_labels.append(label_s.detach().cpu())\n",
    "signal_outputs=torch.softmax(torch.cat(signal_outputs),axis=1).numpy()\n",
    "signal_labels=np.concatenate(signal_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_outputs=signal_outputs[np.argmax(signal_labels,axis=1)==0]\n",
    "signal_labels_counts=np.unique(np.argmax(signal_outputs,axis=1),return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a5a8e",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.fill_between(np.arange(args.NUM_CLASSES-1),0,bg_labels_counts,alpha=1,hatch='/', facecolor=\"skyblue\")\n",
    "# plt.fill_between(np.arange(args.NUM_CLASSES-1),bg_labels_counts,bg_labels_counts+signal_labels_counts,color='red',alpha=1)\n",
    "#plt.fill_between(np.arange(args.NUM_CLASSES-1),outputs.mean(axis=0)[:args.NUM_CLASSES-1],outputs.mean(axis=0)[:args.NUM_CLASSES-1]+signal_outputs[np.argmax(signal_labels,axis=1)==0].mean(axis=0)[:args.NUM_CLASSES-1],color='red',alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb9962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe; enumerate is used to make column names\n",
    "columns=['background','signal']\n",
    "fig,ax=plt.subplots(dpi=150)\n",
    "df = pd.concat([pd.DataFrame(a, columns=[columns[i]]) for i, a in enumerate([bg_outputs, np.argmax(signal_outputs,axis=1)], 0)], axis=1)\n",
    "\n",
    "# plot the data\n",
    "df.plot.hist(stacked=True, bins=args.NUM_CLASSES-1,ax=ax, color = ['skyblue','red'])\n",
    "ax.set_xlim(0,args.NUM_CLASSES-2)\n",
    "# ax.set_xticks(np.arange(args.NUM_CLASSES-1))\n",
    "# ax.set_xticklabels((np.linspace(0,2,args.NUM_CLASSES-1)*np.pi))\n",
    "#bars = ax.patches\n",
    "# hatches = ['/','\\\\']\n",
    "\n",
    "# for i in range(2):\n",
    "#     for j in range(args.NUM_CLASSES-1):\n",
    "#         bars[i*(args.NUM_CLASSES-1)+j].set_hatch(hatches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b8d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcbddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
